{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from TREMBA.dataloader import imagenet\n",
    "from imagenet_labels import imagenet_labels\n",
    "from itertools import product \n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "success_threshold = 0.5\n",
    "\n",
    "################## CONCAT ALL RESULTS ########\n",
    "root_path = 'results'\n",
    "results = [os.path.join(root, name)\n",
    "for root, dirs, files in os.walk(root_path)\n",
    "for name in files\n",
    "if name.endswith(\"eval_summary.csv\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Determine all combinations of conv and eval models\n",
    "conv_models = []\n",
    "eval_models = []\n",
    "epsilons = []\n",
    "for r in results:\n",
    "    if '/' in r:\n",
    "        base_name = r.split(\"/\")[1]\n",
    "    else:\n",
    "        base_name = r.split(\"\\\\\")[1]\n",
    "    \n",
    "    conv_model = base_name.split(\"_\")[0]\n",
    "    eval_model = base_name.split(\".\")[1].split(\"_\")[1]\n",
    "    epsilon = base_name.split(\"_eval\")[0].split(\"_\")[-1]\n",
    "    \n",
    "    if conv_model not in conv_models:\n",
    "        conv_models.append(conv_model)\n",
    "    \n",
    "    if eval_model not in eval_models:\n",
    "        eval_models.append(eval_model)\n",
    "    \n",
    "    if epsilon not in epsilons:\n",
    "        epsilons.append(epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter by combination and concat the results \n",
    "combinations = list(product(conv_models, eval_models, epsilons))\n",
    "\n",
    "for c in combinations:\n",
    "    conv_model = c[0]\n",
    "    eval_model = c[1]\n",
    "    epsilon = c[2]\n",
    "\n",
    "    if conv_model == eval_model:\n",
    "        filter_statement = lambda x: (eval_model in x.replace(conv_model, \"\", 1)) and (epsilon in x)\n",
    "    else:\n",
    "        filter_statement = lambda x: (conv_model in x) and (eval_model in x) and (epsilon in x)\n",
    "\n",
    "    filt = list(filter(filter_statement, results))\n",
    "    if len(filt) == 0:\n",
    "        continue \n",
    "\n",
    "    combined_result_path = f\"{root_path}/{conv_model}_{eval_model}_{epsilon}results.csv\"\n",
    "    \n",
    "    dataframes = [pd.read_csv(f) for f in filt]\n",
    "    concatenated = pd.concat(dataframes)\n",
    "    concatenated['target'] = concatenated['target'].apply(lambda x: imagenet_labels[x])\n",
    "    concatenated.to_csv(combined_result_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################## LOAD RESULTS AND SCORE SEMANTICALLY #########\n",
    "\n",
    "root_path = 'results'\n",
    "results = [os.path.join(root, name)\n",
    "for root, dirs, files in os.walk(root_path)\n",
    "for name in files\n",
    "if name.endswith(\"results.csv\")]\n",
    "\n",
    "#%%\n",
    "\n",
    "label_mapper = {\n",
    "\n",
    "    \"English springer, English springer spaniel\": \"dog\",\n",
    "    \"chain saw, chainsaw\": \"chainsaw\",\n",
    "    \"French horn, horn\": \"horn\",\n",
    "    \"parachute, chute\": \"parachute\",\n",
    "    \"dog\": \"dog\",\n",
    "}\n",
    "\n",
    "#%%\n",
    "for r in results:\n",
    "    if 'final' in r:\n",
    "        continue \n",
    "    \n",
    "    df = pd.read_csv(r)\n",
    "    df['target'] = df['target'].apply(lambda x: label_mapper[x])\n",
    "\n",
    "    emb1 = model.encode(df['target'], convert_to_tensor=True)\n",
    "    emb2 = model.encode(df['adversarial_caption_np'], convert_to_tensor=True)\n",
    "    cos_sim = util.pytorch_cos_sim(emb1, emb2)\n",
    "    \n",
    "    df['score'] = np.diagonal(cos_sim.detach().cpu().numpy())\n",
    "    df['success'] = df['score'].apply(lambda x: 1 if x >= success_threshold else 0)\n",
    "\n",
    "    save_path = r[:-4] + \"_scored.csv\"\n",
    "    df.to_csv(save_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load scored results and tabulate them by source, eval models + epsilon + target and show success rates \n",
    "\n",
    "root_path = 'results'\n",
    "results = [os.path.join(root, name)\n",
    "for root, dirs, files in os.walk(root_path)\n",
    "for name in files\n",
    "if name.endswith(\"scored.csv\")]\n",
    "# %%\n",
    "dfs = []\n",
    "for r in results:\n",
    "    df = pd.read_csv(r)\n",
    "    dfs.append(df)\n",
    "fulldf = pd.concat(dfs)\n",
    "# %%\n",
    "result_df_rows = []\n",
    "for name, group in fulldf.groupby(['source_model', 'eval_model', 'k', 'target']):\n",
    "    success_rate = group['success'].mean()\n",
    "\n",
    "    row = [*name, success_rate]\n",
    "    result_df_rows.append(row)\n",
    "\n",
    "result_df = pd.DataFrame(result_df_rows, columns=['source_model', 'eval_model', 'k', 'target', 'success_rate'])\n",
    "# %%\n",
    "result_df.to_csv(\"results/final_results.csv\", index=False)\n",
    "# %%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12 (default, Oct 12 2021, 03:01:40) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03a19f8fa65b74cd7dae413f97208843b288dfc75587b9c5e0b97b3a37e45809"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
